고차원 데이터에 대해 GPU 기반 K-means 알고리즘을 구현할 때, 클러스터 중심점(centroids)을 shared memory에 저장하는 데 있어 물리적인 제약이 존재한다. 특히 $K \times D$크기의 centroid 행렬은 shared memory의 용량을 초과할 수 있어, 기존 접근 방식으로는 효율적인 클러스터링이 불가능하다.

FastK는 이러한 문제를 해결하기 위해, feature dimension 축을 여러 개의 chunk로 분할하여 shared memory에 순차적으로 적재하는 구조를 도입하였다. 각 chunk는 제한된 크기의 shared memory 내에서 centroid의 일부분만 로딩되며, 해당 chunk 범위 내에서 거리 계산과 클러스터 할당이 병렬로 수행된다. 이후 다음 dimension chunk를 로딩하여 동일한 과정을 반복함으로써 전체 연산을 완성한다.
유클리드 거리 계산에서 각 차원은 서로에 대해 독립적이기 때문에 일관성을 위배하지 않는다. 또한 차원 축을 일정한 크기로 chunking하여 shared memory에 올리기 때문에 마치 "sliding wnd"처럼 사용하여 cache 효율성을 높이는 구조로 만들 수 있다.

이 방식은 다음과 같은 이점을 제공한다:
- Shared memory 활용 극대화: shared memory에 모두 적재할 수 없는 고차원 centroid도 chunk로 나눠 처리 가능.
- Global memory access 최소화: 자주 접근하는 centroid 정보를 shared memory에 유지함으로써 bandwidth 병목 완화.
- Scalable to any D: 차원이 10K 이상이더라도 chunk 크기만 조절하면 동일 구조로 처리 가능.
    
이는 기존의 sample 중심 병렬화 방식과 달리, dimension 축에 대한 병렬화와 메모리 최적화 전략을 결합한 독창적인 설계로, 특히 고차원 공간에서의 효율적인 클러스터링 수행을 가능하게 한다.


One of the primary bottlenecks in GPU-accelerated k-means clustering for high-dimensional data arises from the limited size of shared memory available per thread block. When the dimensionality $D$ of the feature space is large, the total memory footprint of cluster centroids-represented as a $K \times D$ matrix-can easily exceed the shared memory capacity, especially when the number of clusters $K$ is also large. This constraint forces the algorithm to rely on global memory access for centroid information, leading to significant latency and reduced parallel efficiency.

To address this challenge, FastK introduces a dimension-wise chunking strategy that enables effective use of shared memory without sacrificing accuracy or scalability. Instead of loading the entire centroid matrix into shared memory at once, the algorithm partitions the feature dimensions into fixed-size chunks that fit within the shared memory budget. For each chunk, the corresponding subspace of all centroids is loaded into shared memory and used to compute partial distances in parallel. These partial distances are then accumulated across all chunks to complete the full Euclidean distance computation between data samples and centroids.

Since each dimension contributes independently to the Euclidean distance, splitting the computation across dimension chunks does not violate mathematical consistency. Moreover, by sliding fixed-size chunks of dimensions into shared memory in sequence, the method effectively functions like a sliding window, enhancing memory locality and increasing shared memory reuse, which leads to improved cache efficiency.


This approach offers several advantages:
- Memory scalability: By controlling the chunk size based on the available shared memory, FastK supports arbitrarily high-dimensional data without exhausting shared memory resources.
    
- Reduced memory traffic: Since the centroid data is reused multiple times within each chunk, transferring it to fast shared memory significantly reduces the number of global memory accesses.
    
- Parallel efficiency: Each thread in a block operates on a portion of the chunked data, allowing full utilization of GPU cores even in extreme dimensions.

In practice, this chunking mechanism enables FastK to operate efficiently on datasets with tens of thousands of feature dimensions-a regime where traditional GPU implementations of k-means fail due to memory limitations. The chunk size is automatically computed at runtime based on the GPU’s hardware specifications (e.g., shared memory per block), ensuring portability and robustness across different GPU architectures.