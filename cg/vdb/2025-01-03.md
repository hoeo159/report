# 해야 할 것들

1. TPB
2. chunk_size
3. cluster K 확장

Cuda Device 사양 확인
```
cudaDeviceProp prop;
cudaGetDeviceProperties(&prop, 0);
```
chunkSize는 shared memory의 공간을 넘을 수 없고 너무 작으면 성능이 하락했다. `prop.sharedMemPerBlock`에서 `K * sizeof(float)` 만큼 나누어서 32 배수 만큼의 최대 chunkSize를 가져왔다. TPB 역시 chunkSize의 최대 비트를 가져와서 2의 제곱수 중 가장 큰 수를 선택했다. TPB가 chunkSize를 넘어갈 때 가끔 한 번씩 시간이 튀어오르는 경우가 생겼기 때문.
```
size_t maxChunkSize = prop.sharedMemPerBlock / (K * sizeof(float));
int chunkSize = (int)maxChunkSize;
chunkSize = (chunkSize / 32) * 32;
int TPB = std::__bit_floor(chunkSize);
```

([nvidia](https://forums.developer.nvidia.com/t/a-block-size-less-than-32/67691)나 [stack](https://stackoverflow.com/questions/55344153/cuda-execution-time-compared-to-block-size)에서 blocksize에 대한 관련 글들을 보았을 때 TPB가 32 이상일 때는 성능 변화가 flatten하고 32보다 작을 때 성능이 오히려 더 좋은 경우도 생기는 것으로 보아 best solution을 찾으려면 각 디바이스마다 실험값을 측정해야 한다.)

K가 높아질 수록 cluster 간 간격이 좁아져 data 개수가 적을 때 다른 cluster에게 뺏기면 계속 0이 되는 현상이 반복되고 있다.
해당 과정까지 진행했을 때 chunkSize를 32까지 놓는다면 372개의 K까지 돌아가긴 한다.

앞으로 할 것 : 
K가 1000개 정도가 생길 때 이들을 분할해서 clustering을 진행해야 할 수도 있다.

Kmeans
	loop : 끝나는 조건 추가해야 함
		1. 각 포인트가 가장 가까운 cluster를 찾아 라벨링
		2. cluster N에 소속되어있는 포인트끼리 찾아서 새로운 centroid를 만듦(부분합)
		3. 부분합 과정을 전체 size로 나눔

여기서 K가 너무 큰 경우 클러스터를 쪼갠다???????

라벨링은 모든 포인트, 모든 클러스터에 대해서 진행
클러스터를 update할 때, 200개의 클러스터만 업데이트하기


블록 내부에서 atomic 없이 합산 → 대표 스레드가 전역에 한 번에 기록??


